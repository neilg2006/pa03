3 6         #number of layers, number of nodes
2 identity  #nodes in layer 0 (input)
3 sigmoid   #nodes in layer 1 and applies ReLU activation
1 sigmoid   #nodes in layer 2 (output) and applies sigmoid activation
9           #number of weights to load
0 2 -0.549746
0 3 -1.40287
0 4 1.58275
1 2 -1.04515
1 3 0.257594
1 4 -1.95939
2 5 -1.50781
3 5 -0.315292
4 5 0.858179
0           #number of biases to load